# -*- coding: utf-8 -*-
"""HAR using MobileNet on UCF .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cWiRPkkG8vD08uZ0cdwLiZj_jarwhkWe
"""

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/My Drive/HAR LRCN FINAL

import pandas as pd
import numpy as np
import cv2
import os
import h5py
from tqdm import tqdm
from keras.preprocessing import image
from keras.applications.inception_v3 import InceptionV3, preprocess_input
from keras.applications.densenet import *
from keras.models import Model, load_model, Sequential
from keras.layers import Input, LSTM, Dense, Dropout
from keras.utils import to_categorical
from keras.applications.imagenet_utils import preprocess_input
from keras.layers.normalization import BatchNormalization
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, TensorBoard,EarlyStopping
from keras.utils.io_utils import HDF5Matrix
import matplotlib.pyplot as plt
from keras.layers.wrappers import TimeDistributed
from keras.layers.convolutional import (Conv2D, MaxPooling3D, Conv3D,MaxPooling2D)
from keras.models import Sequential
from keras.layers import Dense, Activation,Dropout
from keras.layers import LSTM

#Video Reader Class to get frames in a desired manner#

import numpy as np
from skvideo.io import FFmpegReader, ffprobe
from skvideo.utils import rgb2gray
from PIL import Image
from keras.preprocessing import image
from tqdm import tqdm


class Videos(object):

    def __init__(self, target_size=None, to_gray=True, max_frames=None,
                 extract_frames='middle', required_fps=None,
                 normalize_pixels=None):
        """
        Initializing the config variables

        Parameters:
            target_size (tuple): (New_Width, New_Height), Default 'None'
                A tuple denoting the target width and height of each frame in each of the video

            to_gray (boolean): Default 'True'
                If True, then each frame will be converted to gray scale. Otherwise, not.

            max_frames (int): Default 'None'
                The maximum number of frames to return for each video.
                Extra frames are removed based on the value of 'extract_frames'.

            extract_frames (str): {'first', 'middle', 'last'}, Default 'middle'
                'first': Extract the first 'N' frames

                'last': Extract the last 'N' frames

                'middle': Extract 'N' frames from the middle
                    Remove ((total_frames - max_frames) // 2) frames from the beginning as well as the end

            required_fps (int): Default 'None'
                Capture 'N' frame(s) per second from the video.

                Only the first 'N' frame(s) for each second in the video are captured.

            normalize_pixels (tuple/str): Default 'None'
                If 'None', the pixels will not be normalized.

                If a tuple - (New_min, New_max) is passed, Min-max Normalization will be used.

                If the value is 'z-score', then Z-score Normalization will be used.
                For each pixel p, z_score = (p - mean) / std
        """

        self.target_size = target_size
        self.to_gray = to_gray
        self.max_frames = max_frames
        self.extract_frames = extract_frames
        self.required_fps = required_fps
        self.normalize_pixels = normalize_pixels
        self.fps = None

    def read_videos(self, paths):
        """
        Parameters:
            paths (list): Required
                 A list of paths of the videos to be read

        Returns:
            Numpy.ndarray
                A 5-d tensor with shape (<No. of Videos>, <No. of frames>, <height>, <width>, <channels>)
        """

        list_of_videos = [
            self._read_video(path) for path in tqdm(paths)
        ]

        tensor = np.vstack(list_of_videos)

        if self.normalize_pixels != None:
            # Pixels are normalized for each video individually
            if (type(self.normalize_pixels) == tuple) and (len(self.normalize_pixels) == 2):
                base = self.normalize_pixels[0]
                r = self.normalize_pixels[1] - base
                min_ = np.min(tensor, axis=(1, 2, 3), keepdims=True)
                max_ = np.max(tensor, axis=(1, 2, 3), keepdims=True)
                return ((tensor.astype('float32') - min_) / (max_ - min_)) * r + base

            elif self.normalize_pixels == 'z-score':
                mean = np.mean(tensor, axis=(1, 2, 3), keepdims=True)
                std = np.std(tensor, axis=(1, 2, 3), keepdims=True)
                return (tensor.astype('float32') - mean) / std
            
            else:
                raise ValueError('Invalid value of \'normalize_pixels\'')

        return tensor

    def get_frame_count(self, paths):
        """
        Can be used to determine the value of `max_frames`

        Parameters:
            paths (list): Required
                 A list of paths of the videos to be read

        Returns:
            dict (python dictionary)
                For each video, the total number of frames in that video is stored in the dictionary.
        """

        frame_count = {}
        for path in paths:
            cap = FFmpegReader(filename=path)
            frame_count[path] = cap.inputframenum
            cap.close()

        return frame_count

    def _read_video(self, path):
        """
        Parameters:
            path (str): Required
                Path of the video to be read

        Returns:
            Numpy.ndarray
                A 5-d tensor with shape (1, <No. of frames>, <height>, <width>, <channels>)
        """

        cap = FFmpegReader(filename=path)
        list_of_frames = []
        self.fps = int(cap.inputfps)                  # Frame Rate

        for index, frame in enumerate(cap.nextFrame()):

            capture_frame = True
            if self.required_fps != None:
                is_valid = range(self.required_fps)
                capture_frame = (index % self.fps) in is_valid

            if capture_frame:

                if self.target_size is not None:
                    temp_image = image.array_to_img(frame)
                    frame = image.img_to_array(
                        temp_image.resize(
                            self.target_size,
                            Image.ANTIALIAS)).astype('uint8')

                # Shape of each frame -> (<height>, <width>, 3)
                list_of_frames.append(frame)

        temp_video = np.stack(list_of_frames)
        cap.close()

        if self.to_gray:
            temp_video = rgb2gray(temp_video)
                
        if self.max_frames is not None:
            temp_video = self._process_video(video=temp_video)

        return temp_video

    def _process_video(self, video):
        """
        Parameters:
            video (Numpy.ndarray):
                Shape = (<No. of frames>, <height>, <width>, <channels>)

                Video whose frames are to be extracted

        Returns:
            Numpy.ndarray
                A tensor (processed video) with shape (<`max_frames`>, <height>, <width>, <channels>)
        """

        total_frames = video.shape[0]
        if self.max_frames <= total_frames:

            if self.extract_frames == 'first':
                video = video[:self.max_frames]
            elif self.extract_frames == 'last':
                video = video[(total_frames - self.max_frames):]
            elif self.extract_frames == 'middle':
                # No. of frames to remove from the front
                front = ((total_frames - self.max_frames) // 2) + 1
                video = video[front:(front + self.max_frames)]
            else:
                raise ValueError('Invalid value of \'extract_frames\'')

        else:
            raise IndexError(
                'Required number of frames is greater than the total number of frames in the video')

        return video

SEQ_LEN = 36
MAX_SEQ_LEN = 200
BATCH_SIZE = 32
EPOCHS = 1000

firstreader = Videos(target_size=(224, 224), 
                to_gray=False, 
                max_frames=12, 
                extract_frames='first', 
                normalize_pixels=(0, 1))
middlereader = Videos(target_size=(224, 224), 
                to_gray=False, 
                max_frames=12, 
                extract_frames='middle', 
                normalize_pixels=(0, 1))
lastreader = Videos(target_size=(224, 224), 
                to_gray=False, 
                max_frames=12, 
                extract_frames='last', 
                normalize_pixels=(0, 1))

def main():
  
    # Get model with pretrained weights.
    base_model = DenseNet121(weights='imagenet',include_top=True)
    
    
    # We'll extract features at the final pool layer.
    model = Model(inputs=base_model.input,outputs=base_model.get_layer('avg_pool').output)

    
    # Getting the data
    df = get_data('Human-Action-Classification-/data/data_file.csv')
    
    # Clean the data
    df_clean = clean_data(df)
    
    # Creating index-label maps and inverse_maps
    label_index, index_label = get_class_dict(df_clean)
    
    # Split the dataset into train and test
    train, test = split_train_test(df_clean)
    
    # Encoding the dataset
    encode_dataset(train, model, label_index, "train")
    encode_dataset(test,model,label_index,"test")

# def get_data(path, if_pd=False):
#     """Load our data from file."""
#     names = ['partition', 'class', 'video_name', 'frames']
#     df = pd.read_csv(path,names=names)
#     return df

# def get_class_dict(df):
#     class_name =  list(df['class'].unique())
#     index = np.arange(0, len(class_name))
#     label_index = dict(zip(class_name, index))
#     index_label = dict(zip(index, class_name))
#     return (label_index, index_label)
    
# def clean_data(df):
#     mask = np.logical_and(df['frames'] >= SEQ_LEN, df['frames'] <= MAX_SEQ_LEN)
#     df = df[mask]
#     return df
# def split_train_test(df):
#     partition =  (df.groupby(['partition']))
#     un = df['partition'].unique()
#     train = partition.get_group(un[0])
#     test = partition.get_group(un[1])
#     return (train, test)

# def preprocess_image(img):
#     img = cv2.resize(img, (224,224))
#     return preprocess_input(img)

# def pre(img):
#     img = img/255.
#     img -=0.5
#     img *= 2.
#     return img
    
    
# def encode_video(row, model, label_index):
#     cap = cv2.VideoCapture(os.path.join("Human-Action-Classification-/data","UCF-101",str(row["class"].iloc[0]) ,str(row["video_name"].iloc[0]) + ".avi"))
#     images = []  
#     for i in range(SEQ_LEN):
#         ret, frame = cap.read()
#         #frame = np.expand_dims(frame,axis=0)
#         frame = preprocess_image(frame)
#         frame = pre(frame)
#         images.append(frame)
    
#     #print(len(image))
#     features = model.predict(np.array(images))
#     index = label_index[row["class"].iloc[0]]
#     print(index)
#     #y_onehot = to_categorical(index, len(label_index.keys()))
    
#     return features, index

# from keras.utils import np_utils
# def encode_dataset(data, model, label_index, phase):
#     input_f = []
#     output_y = []
#     required_classes = ["ApplyEyeMakeup" , "ApplyLipstick" , "Archery" , "BabyCrawling" , "BalanceBeam" ,
#                        "BandMarching" , "BaseballPitch" , "Basketball" , "BasketballDunk"]
   
    
#     for i in tqdm(range(data.shape[0])):
#     # Check whether the given row , is of a class that is required
#         if str(data.iloc[[i]]["class"].iloc[0]) in required_classes:
 
#             features,y =  encode_video(data.iloc[[i]], model, label_index)
#             input_f.append(features)
#             output_y.append(y)
        
    
#     le_labels = np_utils.to_categorical(output_y)
#     f = h5py.File(phase+'_mobile30fpspre'+'.h5', 'w')
#     f.create_dataset(phase, data=np.array(input_f))
#     f.create_dataset(phase+"_labels", data=np.array(le_labels))
    
#     del input_f[:]
#     del output_y[:]

def get_data(path, if_pd=False):
    """Load our data from file."""
    names = ['partition', 'class', 'video_name', 'frames']
    df = pd.read_csv(path,names=names)
    return df

def get_class_dict(df):
    class_name =  list(df['class'].unique())
    index = np.arange(0, len(class_name))
    label_index = dict(zip(class_name, index))
    index_label = dict(zip(index, class_name))
    return (label_index, index_label)
    
def clean_data(df):
    mask = np.logical_and(df['frames'] >= SEQ_LEN, df['frames'] <= MAX_SEQ_LEN)
    df = df[mask]
    return df
def split_train_test(df):
    partition =  (df.groupby(['partition']))
    un = df['partition'].unique()
    train = partition.get_group(un[0])
    test = partition.get_group(un[1])
    return (train, test)

def preprocess_image(img):
    img = cv2.resize(img, (224,224))
    return preprocess_input(img)

def pre(img):
    img = img/255.
    img -=0.5
    img *= 2.
    return img
    
    
def encode_video(row, model, label_index):
#     cap = cv2.VideoCapture(os.path.join("Human-Action-Classification-/data","UCF-101",str(row["class"].iloc[0]) ,str(row["video_name"].iloc[0]) + ".avi"))
#     images = []
    a = [os.path.join("Human-Action-Classification-/data","UCF-101",str(row["class"].iloc[0]) ,str(row["video_name"].iloc[0]) + ".avi")]
#     for i in range(SEQ_LEN):
#         ret, frame = cap.read()
        #frame = np.expand_dims(frame,axis=0)
#         frame = preprocess_image(frame)
#         frame = pre(frame)
#         images.append(frame)
    vf = firstreader.read_videos(a)
    vm = middlereader.read_videos(a)
    vl = lastreader.read_videos(a)
    images = np.concatenate((vf,vm,vl))
    #print(len(image))
    features = model.predict(images)
    index = label_index[row["class"].iloc[0]]
    print(index)
    #y_onehot = to_categorical(index, len(label_index.keys()))
    
    return features, index

from keras.utils import np_utils
def encode_dataset(data, model, label_index, phase):
    input_f = []
    output_y = []
    required_classes = ["ApplyEyeMakeup" , "ApplyLipstick" , "Archery" , "BabyCrawling" , "BalanceBeam" ,
                       "BandMarching" , "BaseballPitch" , "Basketball" , "BasketballDunk"]
   
    
    for i in tqdm(range(data.shape[0])):
    # Check whether the given row , is of a class that is required
        if str(data.iloc[[i]]["class"].iloc[0]) in required_classes:
 
            features,y =  encode_video(data.iloc[[i]], model, label_index)
            input_f.append(features)
            output_y.append(y)
        
    
    le_labels = np_utils.to_categorical(output_y)
    f = h5py.File(phase+'_mobiletechnique'+'.h5', 'w')
    f.create_dataset(phase, data=np.array(input_f))
    f.create_dataset(phase+"_labels", data=np.array(le_labels))
    
    del input_f[:]
    del output_y[:]

main()

def lstm():
    """Build a simple LSTM network. We pass the extracted features from
    our CNN to this model predominantly."""
    input_shape = (SEQ_LEN, 1024)
    # Model.
    model = Sequential()
    model.add(LSTM(2048,input_shape=input_shape,dropout=0.6))
    model.add(Dense(2048, activation='relu'))
    model.add(Dropout(0.7))
    model.add(Dense(1024, activation='relu'))
    model.add(Dropout(0.8))
    model.add(Dense(9, activation='softmax'))
    
    
     # checkpoint
    filepath="/content/drive/My Drive/HAR LRCN FINAL/mobiletechnique-weight.hdf5"
    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
    callback_list = [checkpoint]
    early_stopping = EarlyStopping(monitor = 'val_loss',patience= 10)
   

    optimizer = Adam(lr=0.0001, decay=1e-6)
    metrics = ['mse','accuracy', 'top_k_categorical_accuracy','categorical_accuracy']
    model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=metrics)
    return model, callback_list

x_train = HDF5Matrix('train_mobiletechnique.h5', 'train')
y_train = HDF5Matrix('train_mobiletechnique.h5', 'train_labels')
x_test = HDF5Matrix('test_mobiletechnique.h5', 'test')
y_test = HDF5Matrix('test_mobiletechnique.h5', 'test_labels')

print(x_train.shape)
print(y_train.shape)
print(y_train[240])
print(x_test.shape)
print(y_test.shape)
model, callback_list = lstm()
model.summary()

# del model
# model = load_model('Activity_Recognition.h5')
# model.load_weights('/content/drive/My Drive/HAR FINAL/best-weight.hdf5')

history = model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs = 200,verbose = 1,validation_data = (x_test, y_test),shuffle = 'batch', callbacks=callback_list)
    
model.save("Activity_Recognition.h5")

model.metrics_names

score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)
print('Test score:', score[0])
print('Test accuracy:', score[1])


model.metrics_names

train_loss=history.history['loss']
val_loss=history.history['val_loss']
train_acc=history.history['acc']
val_acc=history.history['val_acc']
xc=range(200)

plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss)
plt.plot(xc,val_loss)
plt.xlabel('num of Epochs')
plt.ylabel('loss')
plt.title('train_loss vs val_loss')
plt.grid(True)
plt.legend(['train','val'])
print(plt.style.available )
plt.style.use(['classic'])

plt.figure(2,figsize=(7,5))
plt.plot(xc,train_acc)
plt.plot(xc,val_acc)
plt.xlabel('num of Epochs')
plt.ylabel('accuracy')
plt.title('train_acc vs val_acc')
plt.grid(True)
plt.legend(['train','val'],loc=4)

plt.style.use(['classic'])